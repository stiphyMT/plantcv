<<<<<<< HEAD
## Tutorial: VIS Image Pipeline

PlantCV is composed of modular functions that can be arranged (or rearranged) and adjusted quickly and easily.
Pipelines do not need to be linear (and often are not). Please see pipeline example below for more details.
Every function has a optional debug mode that prints out the resulting image. The debug has two modes, either 'plot' or print' if set to
'print' then the function prints the image out, if using a jupyter notebook, you would set debug to plot to have
the images plot images to the screen. Debug mode allows users to visualize and optimize each step on individual test images and small test sets before pipelines are deployed over whole data-sets.

**Workflow**

1.  Optimize pipeline on individual image in debug mode.
2.  Run pipeline on small test set (ideally that spans time and/or treatments).
3.  Re-optimize pipelines on 'problem images' after manual inspection of test set.
4.  Deploy optimized pipeline over test set using parallelization script.

**Running A Pipeline**

To run a VIS pipeline over a single VIS image there are two required inputs:

1.  **Image:** Images can be processed regardless of what type of VIS camera was used (High-throughput platform, digital camera, cell phone camera).
Image processing will work with adjustments if images are well lit and free of background that is similar in color to plant material.  
2.  **Output directory:** If debug mode is on output images from each step are produced, otherwise ~4 final output images are produced.

Optional inputs:  

*  **Result File** file to print results to
*  **Write Image Flag** flag to write out images, otherwise no result images are printed (to save time).
*  **Debug Flag:** Prints an image at each step
*  **Region of Interest:** The user can input their own binary region of interest or image mask (make sure it is the same size as your image or you will have problems).

Sample command to run a pipeline on a single image:  

*  Always test pipelines (preferably with -D flag for debug mode) before running over a full image set

```
./pipelinename.py -i testimg.png -o ./output-images -r results.txt -w -D 'print'
```

### Walk Through A Sample Pipeline

#### Pipelines start by importing necessary packages, and by defining user inputs.

```python
#!/usr/bin/python
import sys, traceback
import cv2
import numpy as np
import argparse
import string
import plantcv as pcv

### Parse command-line arguments
def options():
    parser = argparse.ArgumentParser(description="Imaging processing with opencv")
    parser.add_argument("-i", "--image", help="Input image file.", required=True)
    parser.add_argument("-m", "--roi", help="Input region of interest file.", required=False)
    parser.add_argument("-o", "--outdir", help="Output directory for image files.", required=True)
    parser.add_argument("-D", "--debug", help="Turn on debug, prints intermediate images.", action="store_true")
    args = parser.parse_args()
    return args
```

#### Start of the Main/Customizable portion of the pipeline.

The image input by the user is read in. The device variable is just a counter so that each debug image is labeled in numerical order.

```python
### Main pipeline
def main():
    # Get options
    args = options()
    
    # Read image
    img, path, filename = pcv.readimage(args.image)
    
    # Pipeline step
    device = 0
```

**Figure 1.** Original image.
This particular image was captured by a digital camera, just to show that PlantCV works on images not captured on a 
[high-throughput phenotyping system](http://www.danforthcenter.org/scientists-research/core-technologies/phenotyping) with idealized vis image capture conditions.

![Screenshot](img/tutorial_images/vis/original_image.jpg)
  
In some pipelines (especially ones captured with a high-throughput phenotyping systems, where background is predictable) we first threshold out background.
In this particular pipeline we do some premasking of the background. The goal is to remove as much background as possible without thresholding-out the plant.
In order to perform a binary threshold on an image you need to select one of the color channels H,S,V,L,A,B,R,G,B.
Here we convert the RGB image to HSV colorspace then extract the 's' or saturation channel (see more info [here](rgb2hsv.md)), any channel can be selected based on user need.
If some of the plant is missed or not visible then thresholded channels may be combined (a later step).

```python
    
    debug=args.debug 
    
    # Convert RGB to HSV and extract the Saturation channel
    device, s = pcv.rgb2gray_hsv(img, 's', device, debug)
```

**Figure 2.** Saturation channel from original RGB image converted to HSV colorspace.

![Screenshot](img/tutorial_images/vis/01_hsv_saturation.jpg)

Next, the saturation channel is thresholded.
The threshold can be on either light or dark objects in the image (see more info on threshold function [here](binary_threshold.md)).

Tip: This step is often one that needs to be adjusted depending on the lighting and configurations of your camera system

```python
    # Threshold the Saturation image
    device, s_thresh = pcv.binary_threshold(s, 85, 255, 'light', device, debug)
```

**Figure 3.** Thresholded saturation channel image (Figure 2). Remaining objects are in white.

![Screenshot](img/tutorial_images/vis/02_binary_threshold85.jpg)

Again depending on the lighting, it will be possible to remove more/less background.
A median blur (more info [here](median_blur.md)) can be used to remove noise.

Tip: Fill and median blur type steps should be used as sparingly as possible. Depending on the plant type (esp. grasses with thin leaves that often twist) you can lose plant material with a median blur that is too harsh.

```python
    # Median Filter
    device, s_mblur = pcv.median_blur(s_thresh, 5, device, debug)
    device, s_cnt = pcv.median_blur(s_thresh, 5, device, debug)
```

**Figure 4.** Thresholded saturation channel image with median blur.

![Screenshot](img/tutorial_images/vis/03_median_blur5.jpg)

Here is where the pipeline branches.
The original image is used again to select the blue-yellow channel from LAB colorspace (more info on the function [here](rgb2lab.md)).
This image is again thresholded and there is an optional fill step that wasn't needed in this pipeline.

```python
    # Convert RGB to LAB and extract the Blue channel
    device, b = pcv.rgb2gray_lab(img, 'b', device, debug)
    
    # Threshold the blue image
    device, b_thresh = pcv.binary_threshold(b, 160, 255, 'light', device, debug)
    device, b_cnt = pcv.binary_threshold(b, 160, 255, 'light', device, debug)
    
    # Fill small objects
    #device, b_fill = pcv.fill(b_thresh, b_cnt, 10, device, debug)
```

**Figure 5.** (Top) Blue-yellow channel from LAB colorspace from original image (Top). (Bottom) Thresholded blue-yellow channel image.

![Screenshot](img/tutorial_images/vis/05_lab_blue-yellow.jpg)

![Screenshot](img/tutorial_images/vis/06_binary_threshold160.jpg)

Join the binary images from Figure 4 and Figure 5 with the Logical Or function (for more info on the Logical Or function see [here](logical_or.md))

```python
    # Join the thresholded saturation and blue-yellow images
    device, bs = pcv.logical_or(s_mblur, b_cnt, device, debug)
```

**Figure 6.** Joined binary images (Figure 4 and Figure 5).

![Screenshot](img/tutorial_images/vis/08_or_joined.jpg)

Next, apply the binary image (Figure 6) as an image mask over the original image (For more info on mask function see [here](apply_mask.md).
The point of this mask is really to exclude as much background with simple thresholding without leaving out plant material.

```python
    # Apply Mask (for vis images, mask_color=white)
    device, masked = pcv.apply_mask(img, bs, 'white', device, debug)
```

**Figure 7.** Masked image with background removed.

![Screenshot](img/tutorial_images/vis/09_wmasked.jpg)

Now we'll focus on capturing the plant in the masked image from Figure 7.
The masked green-magenta and blue-yellow channels are extracted.
Then the two channels are thresholded to capture different portions of the plant and the three images are joined together.
The small objects are filled (for more info on the fill function see [here](fill.md)).
The resulting binary image is used to mask the masked image from Figure 7.

```python
    # Convert RGB to LAB and extract the Green-Magenta and Blue-Yellow channels
    device, masked_a = pcv.rgb2gray_lab(masked, 'a', device, debug)
    device, masked_b = pcv.rgb2gray_lab(masked, 'b', device, debug)
    
    # Threshold the green-magenta and blue images
    device, maskeda_thresh = pcv.binary_threshold(masked_a, 115, 255, 'dark', device, debug)
    device, maskeda_thresh1 = pcv.binary_threshold(masked_a, 135, 255, 'light', device, debug)
    device, maskedb_thresh = pcv.binary_threshold(masked_b, 128, 255, 'light', device, debug)
    
    # Join the thresholded saturation and blue-yellow images (OR)
    device, ab1 = pcv.logical_or(maskeda_thresh, maskedb_thresh, device, debug)
    device, ab = pcv.logical_or(maskeda_thresh1, ab1, device, debug)
    device, ab_cnt = pcv.logical_or(maskeda_thresh1, ab1, device, debug)
    
    # Fill small objects
    device, ab_fill = pcv.fill(ab, ab_cnt, 200, device, debug)
    
    # Apply mask (for vis images, mask_color=white)
    device, masked2 = pcv.apply_mask(masked, ab_fill, 'white', device, debug)
```

The sample image used had very green leaves, but often (especially with stress treatments) 
there are yellowing or redish leaves or regions of necrosis. The different thresholded 
channels capture different regions of the plant, then are combined into a mask for the image 
that was previously masked (Figure 7).

**Figure 8.** RGB to LAB conversion. (Top) The green-magenta "a" channel image. (Bottom) The blue-yellow "b" channel image.

![Screenshot](img/tutorial_images/vis/10_lab_green-magenta.jpg)

![Screenshot](img/tutorial_images/vis/11_lab_blue-yellow.jpg)

**Figure 9.** Thresholded LAB channel images. (Top) "dark" threshold 115. (Middle) "light" threshold 135. (Bottom) "light" threshold 128.

![Screenshot](img/tutorial_images/vis/12_binary_threshold115_inv.jpg)

![Screenshot](img/tutorial_images/vis/13_binary_threshold135.jpg)

![Screenshot](img/tutorial_images/vis/14_binary_threshold128.jpg)

**Figure 9.** Combined thresholded images.

![Screenshot](img/tutorial_images/vis/15_or_joined.jpg)

![Screenshot](img/tutorial_images/vis/16_or_joined.jpg)

![Screenshot](img/tutorial_images/vis/17_or_joined.jpg)

 **Figure 10.** Fill in small objects. (Top) Image with objects < 200 px filled. (Bottom) Masked image.
 
![Screenshot](img/tutorial_images/vis/18_fill200.jpg)

![Screenshot](img/tutorial_images/vis/19_wmasked.jpg)

Now we need to identify the objects (called contours in OpenCV) within the image. 
For more information on this function see [here](find_objects.md)

```python
    # Identify objects
    device, id_objects,obj_hierarchy = pcv.find_objects(masked2, ab_fill, device, debug)
```

**Figure 11.** Here the objects (purple) are identified from the image from Figure 10.
Even the spaces within an object are colored, but will have different hierarchy values.

![Screenshot](img/tutorial_images/vis/20_id_objects.jpg)

Next the region of interest is defined (this can be made on the fly, for more information 
see [here](define_roi.md))

```python
    # Define ROI
    device, roi1, roi_hierarchy= pcv.define_roi(masked2, 'rectangle', device, None, 'default', debug, True, 550, 0, -500, -1900)
```

**Figure 12.** Region of interest drawn onto image. 

![Screenshot](img/tutorial_images/vis/21_roi.jpg)

Once the region of interest is defined you can decide to keep all of the contained 
and overlapping with that region of interest or cut the objects to the shape of the region of interest.
For more information see [here](roi_objects.md).

```python
    # Decide which objects to keep
    device,roi_objects, hierarchy3, kept_mask, obj_area = pcv.roi_objects(img, 'partial', roi1, roi_hierarchy, id_objects, obj_hierarchy, device, debug)
```

**Figure 13.** Kept objects (green) drawn onto image.

![Screenshot](img/tutorial_images/vis/22_obj_on_img.jpg)

The isolated objects now should all be plant material. There, can however, 
be more than one object that makes up a plant, since sometimes leaves twist 
making them appear in images as seperate objects. Therefore, in order for 
shape analysis to perform properly the plant objects need to be combined into 
one object using the Combine Objects function (for more info see [here](object_composition.md)).

```python
    # Object combine kept objects
    device, obj, mask = pcv.object_composition(img, roi_objects, hierarchy3, device, debug)
```

**Figure 14.** Outline (blue) of combined objects on the image. 

![Screenshot](img/tutorial_images/vis/23_objcomp.jpg)

The next step is to analyze the plant object for traits such as shape, or color.
For more info see the Shape Function [here](analyze_shape.md),
the Color Function [here](analyze_color.md),
and the Boundary tool function [here](analyze_bound.md).

```python
############### Analysis ################
  
    outfile=False
    if args.writeimg==True:
        outfile=args.outdir+"/"+filename
  
    # Find shape properties, output shape image (optional)
    device, shape_header, shape_data, shape_img = pcv.analyze_object(img, args.image, obj, mask, device, debug, args.outdir + '/' + filename)
    
    # Shape properties relative to user boundary line (optional)
    device, boundary_header, boundary_data, boundary_img1 = pcv.analyze_bound(img, args.image, obj, mask, 1680, device, debug, args.outdir + '/' + filename)
    
    # Determine color properties: Histograms, Color Slices and Pseudocolored Images, output color analyzed images (optional)
    device, color_header, color_data, norm_slice = pcv.analyze_color(img, args.image, kept_mask, 256, device, debug, 'all', 'rgb', 'v', 'img', 300, args.outdir + '/' + filename)
    
    # Write shape and color data to results file
    result=open(args.result,"a")
    result.write('\t'.join(map(str,shape_header)))
    result.write("\n")
    result.write('\t'.join(map(str,shape_data)))
    result.write("\n")
    for row in shape_img:  
        result.write('\t'.join(map(str,row)))
        result.write("\n")
    result.write('\t'.join(map(str,color_header)))
    result.write("\n")
    result.write('\t'.join(map(str,color_data)))
    result.write("\n")
    for row in color_img:
        result.write('\t'.join(map(str,row)))
        result.write("\n")
    result.close()
  
if __name__ == '__main__':
    main()
```

**Figure 15.** Shape analysis output image.

![Screenshot](img/tutorial_images/vis/24_shapes.jpg)

**Figure 16.** Boundary line output image.

![Screenshot](img/tutorial_images/vis/25_boundary.jpg)

**Figure 17.** Pseudocolored image (based on value channel).

![Screenshot](img/tutorial_images/vis/26_pseudo_on_img.jpg)

**Figure 18.** Histogram of color values for each plant pixel.

![Screenshot](img/tutorial_images/vis/27_all_hist.jpg)

### Additional examples

To demonstrate the importance of camera settings on pipeline construction, 
here are different species of plants captured with the same imaging setup 
(digital camera) and processed with the same imaging pipeline as above (no settings changed).

**Figure 19.** Output images from Cassava trait analysis. (From top to bottom) Original image, shape output image, boundary line output image, pseudocolored image (based on value channel), histogram of color values for each plant pixel.

![Screenshot](img/tutorial_images/vis/cassava_1_shapes.jpg)

![Screenshot](img/tutorial_images/vis/cassava_2_boundary.jpg)

![Screenshot](img/tutorial_images/vis/cassava_3_pseudo_on_img.jpg)

![Screenshot](img/tutorial_images/vis/cassava_4_all_hist.jpg)

**Figure 20.** Output images from Tomato trait analysis. (From top to bottom) Original image, shape output image, boundary line output image, pseudocolored image (based on value channel), histogram of color values for each plant pixel.

![Screenshot](img/tutorial_images/vis/tomato_1_shapes.jpg)

![Screenshot](img/tutorial_images/vis/tomato_1_shapes.jpg)

![Screenshot](img/tutorial_images/vis/tomato_2_boundary.jpg)

![Screenshot](img/tutorial_images/vis/tomato_3_pseudo_on_img.jpg)

![Screenshot](img/tutorial_images/vis/tomato_4_all_hist.jpg)



To deploy a pipeline over a full image set please see tutorial on 
Pipeline Parallelization [here](pipeline_parallel.md).
||||||| merged common ancestors
=======
## Tutorial: Multi Plant Image Pipeline

PlantCV is composed of modular functions that can be arranged (or rearranged) and adjusted quickly and easily.
Pipelines do not need to be linear (and often are not). Please see pipeline example below for more details.
Every function has a optional debug mode that prints out the resulting image. The debug has two modes, either 'plot' or print' if set to
'print' then the function prints the image out, if using a jupyter notebook, you would set debug to plot to have
the images plot images to the screen. Debug mode allows users to visualize and optimize each step on individual test images and small test sets before pipelines are deployed over whole data-sets.

For multi-plant pipelines, images with multiple plants are processed and result in individual pictures for each plant, allowing a secondary pipeline (see VIS Tutorial for example) to be used. 
The challenge of multi plant processing is that a single plant can be composed of several contours, therefore contours need to be sorted and clustered together in some way.
There are several functions that help with multi-plant image processing. First, the current clustering functions work by asking the user to provide an approximation of the number of desired
'rows' and 'columns' that they would like to split the image into. There does not need to be a plant in each spot, but the grid is used as an approximate region to cluster contours within.
The rotation and shift function allow the image to be moved to optimize accurate clustering. Major assumptions that are made are that plants grow but that the imaging position does not change
drastically. Also, the clustering functions will not work properly once plants start overlapping, since contours would also start overlapping. 

**Workflow**

1.  Optimize pipeline on individual image in debug mode.
2.  Run pipeline on small test set (ideally that spans time and/or treatments).
3.  Re-optimize pipelines on 'problem images' after manual inspection of test set.
4.  Deploy optimized pipeline over test set using parallelization script.

**Running A Pipeline**

To run a Multi Plant pipeline over a single VIS image there are two required inputs:

1.  **Image:** Images can be processed regardless of what type of VIS camera was used (High-throughput platform, digital camera, cell phone camera).
Image processing will work with adjustments if images are well lit and free of background that is similar in color to plant material.  
2.  **Output directory:** If debug mode is on output images from each step are produced, otherwise ~4 final output images are produced.

Optional inputs:  

*  **Names File** path to txt file with names of genotypes to split images into (order of names would be top to bottom, left to right
*  **Debug Flag:** Prints an image at each step

Sample command to run a pipeline on a single image:  

*  Always test pipelines (preferably with -D 'print' option for debug mode) before running over a full image set

```
./pipelinename.py -i multi-plant-img.png -o ./output-images -n names.txt -D 'print'
```

### Walk Through A Sample Pipeline

#### Pipelines start by importing necessary packages, and by defining user inputs.

```python
#!/usr/bin/python

import sys, traceback
import cv2
import os
import re
import numpy as np
import argparse
import string
import plantcv as pcv

### Parse command-line arguments
def options():
    parser = argparse.ArgumentParser(description="Imaging processing with opencv")
    parser.add_argument("-i", "--image", help="Input image file.", required=True)
    parser.add_argument("-o", "--outdir", help="Output directory for image files.", required=True)
    parser.add_argument("-n", "--names", help="path to txt file with names of genotypes to split images into", required =False)
    parser.add_argument("-D", "--debug", help="Turn on debug, prints intermediate images.", action=None)
    args = parser.parse_args()
    return args
```

#### Start of the Main/Customizable portion of the pipeline.

The image input by the user is read in. The device variable is just a counter so that each debug image is labeled in numerical order.

```python
### Main pipeline
def main():
    # Get options
    args = options()
    
    # Read image
    img, path, filename = pcv.readimage(args.image)
    
    debug=args.debug 
    
    # Pipeline step
    device = 0
```

**Figure 1.** Original image.
This particular image was captured by a raspberry pi camera, just to show that PlantCV works on images not captured on a 
[high-throughput phenotyping system](http://www.danforthcenter.org/scientists-research/core-technologies/phenotyping) with idealized vis image capture conditions.
In this dataset images were captured over time of a flat (throughout the day and night).

![Screenshot](img/tutorial_images/multi-img/original-image.jpg)

Check if this is an image captured at night.

```python
 
# Step 1: Check if this is a night image, for some of these datasets images were captured
# at night, even if nothing is visible. To make sure that images are not taken at
# night we check that the image isn't mostly dark (0=black, 255=white).
# if it is a night image it throws a fatal error and stops the pipeline.

if np.average(img) < 50:
    pcv.fatal_error("Night Image")
else:
    pass

```

White balance the image so that color can be compared across images and so that image processing can be the same between images (ideally).
For more information see white balance function [here](white_balance.md).

```python
    
# Step 2: Normalize the white color so you can later
# compare color between images.
# Inputs:
# device = device number. Used to count steps in the workflow
# img = image object, RGB colorspace
# debug = None, print, or plot. Print = save to file, Plot = print to screen.
# roi = region for white reference, if none uses the whole image,
# otherwise (x position, y position, box width, box height)

#white balance image based on white toughspot
device,img1=pcv.white_balance(device,img,debug,roi=(400,800,200,200))

```

**Figure 2.** White balance the image so that later image processing is easier.

![Screenshot](img/tutorial_images/multi-img/2_bmasked.jpg)
![Screenshot](img/tutorial_images/multi-img/2_whitebalance.jpg)
![Screenshot](img/tutorial_images/multi-img/2_whitebalance_roi.jpg)

Rotate image slighly so that plants line up with grid (later step)
For more information see rotate function [here](rotate.md).

```python

# Step 3: Rotate the image 
    
device, rotate_img = pcv.rotate_img(img1, -1, device, debug)
```

**Figure 3.** Rotated image

![Screenshot](img/tutorial_images/multi-img/3_rotated_img.jpg)

Shift image slighly so that plants line up with grid (later step)
For more information see shift function [here](shift.md).

```python

#Step 4: Shift image. This step is important for clustering later on.
# For this image it also allows you to push the green raspberry pi camera
# out of the image. This step might not be necessary for all images.
# The resulting image is the same size as the original.
# Input:
# img = image object
# device = device number. Used to count steps in the workflow
# number = integer, number of pixels to move image
# side = direction to move from "top", "bottom", "right","left"
# debug = None, print, or plot. Print = save to file, Plot = print to screen.

device, shift1 = pcv.shift_img(img1, device, 300, 'top', debug)
img1 = shift1

```

**Figure 4.** Shifted image

![Screenshot](img/tutorial_images/multi-img/4_shifted_img.jpg)

Select single color channel that has contrast between the target object and background
For more information see shift function [here](rgb2lab.md).

```python

# STEP 5: Convert image from RGB colorspace to LAB colorspace
# Keep only the green-magenta channel (grayscale)
# Inputs:
#    img     = image object, RGB colorspace
#    channel = color subchannel (l = lightness, a = green-magenta , b = blue-yellow)
#    device  = device number. Used to count steps in the workflow
#    debug   = None, print, or plot. Print = save to file, Plot = print to screen.
device, a = pcv.rgb2gray_lab(img1, 'a', device, debug)

```

**Figure 5.** (Top) Green-magenta channel from LAB colorspace from original image.

![Screenshot](img/tutorial_images/multi-img/5_lab_green-magenta.jpg)

Threshold green-magenta image. For more information see binary threshold function [here](binary_threshold.md).

```python

# STEP 6: Set a binary threshold on the Saturation channel image
# Inputs:
#    img         = img object, grayscale
#    threshold   = threshold value (0-255)
#    maxValue    = value to apply above threshold (usually 255 = white)
#    object_type = light or dark
#                  - If object is light then standard thresholding is done
#                  - If object is dark then inverse thresholding is done
#    device      = device number. Used to count steps in the pipeline
#    debug       = None, print, or plot. Print = save to file, Plot = print to screen.
device, img_binary = pcv.binary_threshold(a, 120, 255, 'dark', device, debug)
#                                            ^
#                                            |
#                                           adjust this value

```

**Figure 6.** Thresholded image.

![Screenshot](img/tutorial_images/multi-img/6_binary_threshold120_inv.jpg)

Fill noise. For more information on this function see [here](fill.md)

```python

# STEP 7: Fill in small objects (speckles)
# Inputs:
#    img    = image object, grayscale. img will be returned after filling
#    mask   = image object, grayscale. This image will be used to identify contours
#    size   = minimum object area size in pixels (integer)
#    device = device number. Used to count steps in the pipeline
#    debug  = None, print, or plot. Print = save to file, Plot = print to screen.
mask = np.copy(img_binary)
device, fill_image = pcv.fill(img_binary, mask, 100, device, debug)
#                                               ^
#                                               |
#                                               adjust this value

```

**Figure 7.** Fill noise.

![Screenshot](img/tutorial_images/multi-img/7_fill100.jpg)

Dilate binary image. For more information on this function see [here](dilate.md)

```python

# STEP 8: Dilate so that you don't lose leaves (just in case)
# Inputs:
#    img     = input image
#    kernel  = integer
#    i       = interations, i.e. number of consecutive filtering passes
#    device  = device number. Used to count steps in the pipeline
#    debug   = None, print, or plot. Print = save to file, Plot = print to screen.

device, dilated = pcv.dilate(fill_image, 1, 1, device, debug)

```

**Figure 8.** Dilated image.

![Screenshot](img/tutorial_images/multi-img/8_dil_image_itr_1.jpg)

Find objects within the image, for more information 
see [here](find_objects.md))

```python

# STEP 9: Find objects (contours: black-white boundaries)
# Inputs:
#    img       = image that the objects will be overlayed
#    mask      = what is used for object detection
#    device    = device number.  Used to count steps in the pipeline
#    debug     = None, print, or plot. Print = save to file, Plot = print to screen.
device, id_objects, obj_hierarchy = pcv.find_objects(img1, dilated, device, debug)

```

**Figure 9.** Identified objects. 

![Screenshot](img/tutorial_images/multi-img/9_id_objects.jpg)

Define region of interest in the image, for more information see [here](define_roi.md).

```python

# STEP 10: Define region of interest (ROI)
# Inputs:
#    img       = img to overlay roi
#    roi       = default (None) or user input ROI image, object area should be white and background should be black,
#                has not been optimized for more than one ROI
#    roi_input = type of file roi_base is, either 'binary', 'rgb', or 'default' (no ROI inputted)
#    shape     = desired shape of final roi, either 'rectangle' or 'circle', if  user inputs rectangular roi but chooses
#                'circle' for shape then a circle is fitted around rectangular roi (and vice versa)
#    device    = device number.  Used to count steps in the pipeline
#    debug     = None, print, or plot. Print = save to file, Plot = print to screen.
#    adjust    = either 'True' or 'False', if 'True' allows user to adjust ROI
#    x_adj     = adjust center along x axis
#    y_adj     = adjust center along y axis
#    w_adj     = adjust width
#    h_adj     = adjust height
device, roi, roi_hierarchy = pcv.define_roi(img1, 'rectangle', device, None, 'default', debug, True, 
                                             10, 500, -10, -100)
#                                            ^                ^
#                                            |________________|
#                                            adjust these four values

```

**Figure 10.** Define ROI.

![Screenshot](img/tutorial_images/multi-img/10_roi.jpg)

Once the region of interest is defined you can decide to keep all of the contained 
and overlapping with that region of interest or cut the objects to the shape of the region of interest.
For more information see [here](roi_objects.md).

```python

# STEP 11: Keep objects that overlap with the ROI
# Inputs:
#    img            = img to display kept objects
#    roi_type       = 'cutto' or 'partial' (for partially inside)
#    roi_contour    = contour of roi, output from "View and Ajust ROI" function
#    roi_hierarchy  = contour of roi, output from "View and Ajust ROI" function
#    object_contour = contours of objects, output from "Identifying Objects" fuction
#    obj_hierarchy  = hierarchy of objects, output from "Identifying Objects" fuction
#    device         = device number.  Used to count steps in the pipeline
#    debug          = None, print, or plot. Print = save to file, Plot = print to screen.
device, roi_objects, roi_obj_hierarchy, kept_mask, obj_area = pcv.roi_objects(img1, 'partial', roi, roi_hierarchy,
                                                                           id_objects, obj_hierarchy, device,
                                                                           debug)

```

**Figure 11.** Define ROI.

![Screenshot](img/tutorial_images/multi-img/11_obj_on_img.jpg)
![Screenshot](img/tutorial_images/multi-img/11_roi_mask.jpg)
![Screenshot](img/tutorial_images/multi-img/11_roi_objects.jpg)

Cluster plants based on defined grid, for more info see [here](cluster_contours.md)).

```python

#Step 12: This function take a image with multiple contours and 
# clusters them based on user input of rows and columns

#Inputs:
#    img - An RGB image array
#    roi_objects - object contours in an image that are needed to be clustered.
#    nrow - number of rows to cluster (this should be the approximate  number of desired rows in the entire image (even if there isn't a literal row of plants)
#    ncol - number of columns to cluster (this should be the approximate number of desired columns in the entire image (even if there isn't a literal row of plants)
#    file -  output of filename from read_image function
#    filenames - input txt file with list of filenames in order from top to bottom left to right
#    debug - print debugging images

device, clusters_i, contours = pcv.cluster_contours(device, img1, roi_objects, 4, 6, debug)

```

**Figure 12.** Cluster contours

![Screenshot](img/tutorial_images/multi-img/12_clusters.jpg)

Split the images 

```python

#Step 13:This function takes clustered contours and splits them into multiple images, 
#also does a check to make sure that the number of inputted filenames matches the number
#of clustered contours. If no filenames are given then the objects are just numbered

#Inputs:
#    img - ideally a masked RGB image.
#    grouped_contour_indexes - output of cluster_contours, indexes of clusters of contours
#    contours - contours to cluster, output of cluster_contours
#    file -  the name of the input image to use as a base name , output of filename from read_image function
#    filenames - input txt file with list of filenames in order from top to bottom left to right (likely list of genotypes)
#    debug - print debugging images
    
out = args.outdir
names = args.names
device, output_path = pcv.cluster_contour_splitimg(device, img1, clusters_i, contours, out, file=filename, filenames=names, debug)

```

**Figure 13.** Split image based on clustering.
---
![Screenshot](img/tutorial_images/multi-img/13_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/14_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/15_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/16_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/17_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/18_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/19_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/20_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/21_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/22_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/23_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/24_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/25_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/26_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/27_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/28_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/29_clusters.jpg)
---
![Screenshot](img/tutorial_images/multi-img/30_clusters.jpg)
---
To deploy a pipeline over a full image set please see tutorial on 
Pipeline Parallelization [here](pipeline_parallel.md).
>>>>>>> danforthcenter/master
